import pandas as pd
import sys
import numpy as np
import os
import gzip
from functools import reduce
import scipy.stats as st
import statsmodels.stats.multitest as multi

cohort_nms= ['harvestm12', 'harvestm24','rotterdam1', 'rotterdam2', 'normentfeb', 'normentmay']
smpl_nms= ['maternal','paternal', 'fetal']
batch_nms= ['m12', 'm24']
CHR_nms= [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,12 ,13 ,14 ,15 ,16 ,17 ,18 ,19 ,20 ,21 ,22]
rep_nms= ['normentjan', 'normentjun']


# Other arguments:

pruning_nms= ['none', 'soft', 'moderate', 'hard']

dens_nms= [5]
SNP_nms= [15, 25, 50, 75, 100, 150, 200, 350, 500]
length_nms= [0.0000001]
het_nms= [0, 1]
GAP_nms= [5]

dens_bp= [5000]
SNP_bp= [15, 25, 50, 75, 100, 150, 200, 350, 500]
length_bp= [0.0000001]
het_bp= [0, 1]
GAP_bp= [5000]

# Functions

def isfloat(str):
    try:
        float(str)
        return True
    except ValueError:
        return False

def IVW(df):
	beta_cols= [col for col in df.columns if 'beta' in col]
	sd_cols= [col for col in df.columns if 'sd' in col]
	w= 1/ (df[sd_cols])**2
	beta_meta= np.divide((np.multiply(df[beta_cols], w)).sum(axis= 1), w.sum(axis=1))
	sd_meta= np.divide(1, np.sqrt(w.sum(axis= 1)))
	z_meta= np.divide(beta_meta, sd_meta)
	pvalue_meta= 2*st.norm.cdf(-abs(z_meta))
	meta_df= pd.DataFrame(np.array(list([beta_meta, sd_meta, z_meta, pvalue_meta])).T)
	return meta_df
	
rule high_conf_segments:
	'Obtain a set of high confidence intervals with an FDR< 0.05 and <0.1.'
	input:
		'/mnt/work/pol/ROH/results/eff_num_{sample}.txt',
		expand('/mnt/work/pol/ROH/{cohort}/results/surv_spont_{{sample}}', cohort= cohort_nms)
	output:
		'/mnt/work/pol/ROH/results/HC_{sample}_surv_spont',
		'/mnt/work/pol/ROH/results/LC_{sample}_surv_spont',
		'/mnt/work/pol/ROH/results/NC_{sample}_surv_spont',
		'/mnt/work/pol/ROH/results/meta_{sample}_surv_spont'
	run:
		bon= pd.read_csv(input[0], sep= '\t', header= 0)
		if wildcards.sample != 'maternal': bon= bon.loc[bon.chr != 23, :]
		bon= bon.groupby('chr')['eff_num'].max().sum()
		i= 0
		df_list= list()
		for infile in input[1:]:
			d= pd.read_csv(infile, sep= '\t', header= 0)
			d['segment']= d.chr.map(str) + ':' + d.cM1.map(str) + ':' + d.cM2.map(str)
			d['sd']= np.where(d.beta.isnull(), np.NaN, d.sd)
			d['n']= np.where(d.beta.isnull(), np.NaN, d.n)
			d.rename(columns={'beta': 'beta'+str(i), 'sd': 'sd'+ str(i), 'pos1': 'pos1'+ str(i), 'pos2': 'pos2'+ str(i), 'n': 'n_' + str(i)}, inplace=True)
			d.drop(['cM1', 'cM2', 'chr', 'pvalue'], inplace= True, axis= 1)
			i+= 1
			df_list.append(d)
		df= reduce(lambda x, y: pd.merge(x, y, on = 'segment', how= 'outer'), df_list)
		n_cols= [col for col in df.columns if 'n_' in col]
		R_cols= [col for col in df.columns if 'R' in col and 'pvalue' not in col]
		Rpval_cols= [col for col in df.columns if 'Rpvalue' in col]
		meta_df= IVW(df)
		cols_df= list(df.columns)
		df= pd.concat([df, meta_df], axis=1, ignore_index= True)
		df.columns= cols_df + ['beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta']
		df['n_meta']= df[n_cols].sum(axis=1)
		df= df.loc[df.n_meta>= 10000, :]
		df['pos1']= np.where(pd.notna(df.pos10), df.pos10, np.where(pd.notna(df.pos11), df.pos11, np.where(pd.notna(df.pos12), df.pos12, np.where(pd.notna(df.pos13), df.pos13, np.where(pd.notna(df.pos14), df.pos14,  df.pos15)))))
		df['pos2']= np.where(pd.notna(df.pos20), df.pos20, np.where(pd.notna(df.pos21), df.pos21, np.where(pd.notna(df.pos22), df.pos22, np.where(pd.notna(df.pos23), df.pos23, np.where(pd.notna(df.pos24), df.pos24,  df.pos25)))))
		df.dropna(subset= ['pvalue_meta'], inplace= True)
		dfHC= df.loc[df.pvalue_meta< 0.05/ df.shape[0], :]
		dfLC= df.loc[df.pvalue_meta< 0.1/ df.shape[0], :]
		dfHC.sort_values(by=['pvalue_meta'], inplace= True, ascending= True)
		dfLC.sort_values(by=['pvalue_meta'], inplace= True, ascending= True)
		dfLC= dfLC.iloc[len(dfHC.index):, :]
		dfNC= df.loc[df.pvalue_meta>= 0.1/ df.shape[0], :]
                dfHC.to_csv(output[0], sep= '\t', header= True, index= False, columns= ['segment', 'pos1', 'pos2', 'n_meta', 'beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta'])
                dfLC.to_csv(output[1], sep= '\t', header= True, index= False, columns= ['segment', 'pos1', 'pos2', 'n_meta', 'beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta'])
                dfNC.to_csv(output[2], sep= '\t', header= True, index= False, columns= ['segment', 'pos1', 'pos2', 'n_meta', 'beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta'])
		df.to_csv(output[3], sep= '\t', header= True, index= False, columns= ['segment', 'pos1', 'pos2', 'n_meta', 'beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta'])

rule meta_analysis_imputed:
	'Meta-analysis of imputed variants using inverse variance weighted meta-analysis.'
	input:
		expand('/mnt/work/pol/ROH/{cohort}/results/imputed/imputed_surv_spont_{{sample}}', cohort= cohort_nms)
	output:
		'/mnt/work/pol/ROH/results/imputed/surv_imputed_{sample}.txt'
	run:
		i= 0
		df_list= list()
		for infile in input:
			d= pd.read_csv(infile, delim_whitespace= True , header= None, names= ['variant', 'ref', 'eff', 'n', 'beta','sd', 'pvalue'])
			d['n']= np.where(d.beta.isnull(), np.NaN, d.n)
			d['sd']= np.where(d.beta.isnull(), np.NaN, d.sd)
			d['beta']= np.where([x.strip()[-1] for x in d['variant']] == d['eff'], d['beta'], -1*d['beta'])
			d.rename(columns={'beta': 'beta'+str(i), 'sd': 'sd'+ str(i), 'n': 'n_' + str(i)}, inplace=True)
			d.drop(['pvalue'], inplace= True, axis= 1)
			i+= 1
			df_list.append(d)
		df= reduce(lambda x, y: pd.merge(x, y, on = 'variant', how= 'outer'), df_list)
		n_cols= [col for col in df.columns if 'n' in col]
		meta_df= IVW(df)
		cols_df= list(df.columns)
		beta_cols= [col for col in df.columns if 'beta' in col]
		df= pd.concat([df, meta_df], axis=1, ignore_index= True)
		df.columns= cols_df + ['beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta']
		df['n_meta']= df[n_cols].sum(axis=1)
		df['missing']= df[n_cols].count(axis=1)
		df= df.loc[df.missing> 2, :]
		df.to_csv(output[0], sep= '\t', header= True, index= False, columns= ['variant', 'n_meta', 'beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta'])

rule meta_analysis_replication:
	'IVW of imputed variants.'
	input:
		expand('/mnt/work/pol/ROH/replication/results/imputed_surv_spont_moms_{rep}', rep= rep_nms)
	output:
		'/mnt/work/pol/ROH/results/imputed/replication/surv_moms.txt'
	run:
		i= 0
                df_list= list()
                for infile in input:
                        d= pd.read_csv(infile, sep= '\t', header= None, names= ['variant', 'ref', 'eff', 'n', 'beta','sd', 'pvalue'])
                        d['n']= np.where(d.beta.isnull(), np.NaN, d.n)
                        d['sd']= np.where(d.beta.isnull(), np.NaN, d.sd)
			d['beta']= np.where([x.strip()[-1] for x in d['variant']] == d['eff'], d['beta'], -1*d['beta'])
                        d.rename(columns={'beta': 'beta'+str(i), 'sd': 'sd'+ str(i), 'n': 'n_' + str(i)}, inplace=True)
                        d.drop(['pvalue'], inplace= True, axis= 1)
                        i+= 1
                        df_list.append(d)
		df= reduce(lambda x, y: pd.merge(x, y, on = 'variant', how= 'outer'), df_list)
                n_cols= [col for col in df.columns if 'n' in col]
                meta_df= IVW(df)
                cols_df= list(df.columns)
                df= pd.concat([df, meta_df], axis=1, ignore_index= True)
                df.columns= cols_df + ['beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta']
                df['n_meta']= df[n_cols].sum(axis=1)
                df.to_csv(output[0], sep= '\t', header= True, index= False, columns= ['variant', 'n_meta', 'beta_meta', 'sd_meta', 'z_meta', 'pvalue_meta'])

rule meta_obgyn:
	''
	input:
		expand('/mnt/work/pol/ROH/{cohort}/results/obgyn/obgyn_{{sample}}.txt', cohort= cohort_nms)
	output:
		'/mnt/work/pol/results/obgyn/obgyn_{sample}.txt'
	run:
		i= 0
                df_list= list()
                for infile in input:
                        d= pd.read_csv(infile, sep= '\t', header= None, names= ['Birth_pos', 'forceps', 'prom', 'vacuum','episiotomi', 'assisted', 'segment'])
                        d.rename(columns={'Birth_pos': 'Birth_pos_'+str(i), 'forceps': 'forceps_'+ str(i), 'prom': 'prom_' + str(i), 'vacuum': 'vacuum_' + str(i), 'episiotomi': 'episiotomi_' + str(i), 'assisted': 'assisted_' + str(i)}, inplace=True)
                        i+= 1
                        df_list.append(d)
		df= reduce(lambda x, y: pd.merge(x, y, on = 'segment', how= 'outer'), df_list)
		if df.shape[0] == 0:
			open(output[0], 'a').close()
		else:
			pos_cols= [col for col in df.columns if 'Birth_' in col]
			forc_cols= [col for col in df.columns if 'forceps_' in col]
			prom_cols= [col for col in df.columns if 'prom_' in col]
			vac_cols= [col for col in df.columns if 'vacuum_' in col]
			epi_cols= [col for col in df.columns if 'episiotomi_' in col]
			ass_cols= [col for col in df.columns if 'assisted_' in col]
			df['meta_Birth_pos']= [x[1] for x in df[pos_cols].apply(lambda x: st.combine_pvalues(x, method= 'fisher', weights= None), axis= 1)]
		        df['meta_forceps']= [x[1] for x in df[forc_cols].apply(lambda x: st.combine_pvalues(x, method= 'fisher', weights= None), axis= 1)]
			df['meta_prom']= [x[1] for x in df[prom_cols].apply(lambda x: st.combine_pvalues(x, method= 'fisher', weights= None), axis= 1)]
	                df['meta_vacuum']= [x[1] for x in df[vac_cols].apply(lambda x: st.combine_pvalues(x, method= 'fisher', weights= None), axis= 1)]
		        df['meta_episiotomi']= [x[1] for x in df[epi_cols].apply(lambda x: st.combine_pvalues(x, method= 'fisher', weights= None), axis= 1)]
			df['meta_assisted']= [x[1] for x in df[ass_cols].apply(lambda x: st.combine_pvalues(x, method= 'fisher', weights= None), axis= 1)]
			df= df[['segment', 'meta_Birth_pos', 'meta_forceps', 'meta_prom', 'meta_vacuum', 'meta_episiotomi', 'meta_assisted']]
			df.to_csv(output[0], sep= '\t', header= True, index= False)
	
