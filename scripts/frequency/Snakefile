import pandas as pd
import numpy as np
import os
import gzip
from functools import reduce
import scipy.stats as st
import statsmodels.stats.multitest as multi

cohort_nms= ['harvestm12', 'harvestm24','rotterdam1', 'rotterdam2', 'normentfeb', 'normentmay']
smpl_nms= ['maternal','paternal', 'fetal']
batch_nms= ['m12', 'm24']
CHR_nms= [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,12 ,13 ,14 ,15 ,16 ,17 ,18 ,19 ,20 ,21 ,22]

# Other arguments:

pruning_nms= ['none', 'soft', 'moderate', 'hard']

dens_nms= [5]
SNP_nms= [15, 25, 50, 75, 100, 150, 200, 350, 500]
length_nms= [0.0000001]
het_nms= [0, 1]
GAP_nms= [5]

dens_bp= [5000]
SNP_bp= [15, 25, 50, 75, 100, 150, 200, 350, 500]
length_bp= [0.0000001]
het_bp= [0, 1]
GAP_bp= [5000]

# Functions

def isfloat(str):
    try:
        float(str)
        return True
    except ValueError:
        return False

rule exclude_subjects:
	''
	input:
		'/mnt/work/pol/ROH/pheno/runs_mfr_{sample}.txt',
		'/mnt/work/pol/ROH/genotypes/maps/{sample}/segments_maps_{sample}_chr{CHR}.txt.gz'
	output:
		temp('/mnt/work/pol/ROH/genotypes/maps/{sample}/filtered_map_{sample}_{CHR}.txt')
	run:
		d= pd.read_csv(input[0], sep ='\t', header= 0)
		df= pd.read_csv(input[1], sep ='\t', header= 0, compression= 'gzip')
		cols= ['segment', 'CHR'] + d.IID.values.tolist()
		df= df[cols]
		df.to_csv(output[0], sep= '\t', header= True, index= False)

rule all_segments:
	''
	input:
		expand('/mnt/work/pol/ROH/genotypes/maps/{{sample}}/filtered_map_{{sample}}_{{CHR}}.txt', cohort= cohort_nms)
	output:
		temp('/mnt/work/pol/ROH/runs/segments/all_ROH_frequency_{sample}_{CHR}.txt')
	run:
		df_list= list()
		for i in input:
			x= pd.read_csv(i, sep ='\t', header= 0)
			df_list.append(x)
		d= reduce(lambda x, y: pd.merge(x, y, on = ['CHR', 'segment']), df_list)
		d.to_csv(output[0], sep= '\t', header= True, index= False)

rule ROH_frequency:
	'Calculate ROH frequency.'
	input:
		expand('/mnt/work/pol/ROH/genotypes/maps/{{sample}}/filtered_map_{{sample}}_{CHR}.txt', CHR= CHR_nms)
	output:
		'/mnt/work/pol/ROH/results/ROH_frequency_{sample}.txt'
	run:
		for i in input:
			for chunk in pd.read_csv(i, sep ='\t', header= 0, chunksize= 500, iterator= True):
				chunk.fillna(0, inplace= True)
				x= chunk.iloc[:,2:].mean(axis= 1)
				x1= chunk.iloc[:, 2:].sum(axis= 1)
				x= pd.concat([chunk.iloc[:,0:2], x, x1], axis= 1, ignore_index= True, sort= False)
				x.to_csv(output[0], mode= 'a', sep= '\t', header= False, index= False)

